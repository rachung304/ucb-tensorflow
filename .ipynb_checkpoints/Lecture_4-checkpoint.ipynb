{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning\n",
    "#### The two groups of supervised learning are:\n",
    "- Classification: A classification problem is when the output variable is a category\n",
    "- Regression: A regression problem is when the output variable is a real value\n",
    "\n",
    "#### Some popular examples of supervised machine learning algorithms are:\n",
    "- Linear regression for regression problems\n",
    "- Random Forest for classification and regresion problems\n",
    "- Support vector machines (SVM) for classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning\n",
    "- Unsupervised learning is where you only have input data and no corresponding output variables\n",
    "- The goal for unsupervised learning is to model the underlying stgructure or distribution in the data in order to learn more about the data\n",
    "- These are called unsupervised learning because unlike supervised learning above there is no correct answers and there is not teacher. Algorithms are left to their own devises to discover and present the interesting structure in the data\n",
    "\n",
    "#### Unsupervised Learning problems can be further grouped into clustering and association problems:\n",
    "- Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such grouping customers by puchasing behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-Supervised Machine Learning\n",
    "- Problems where we have a large amount of input data and only some of it is labeled (Y) are called semi-supervised learning problems\n",
    "- These problems sit in between both supervised and unsupervised learning\n",
    "- Hence unlabeled data is cheap and eash to collect and store\n",
    "- You can use supervised learning techniques to discover and learn the structure in the input variables\n",
    "- you can also use supervised learning techniques to make best guess predictions for the unlabeled data, feed that data back into the supervised algorithm as training data and use the model to make predictions on new unseen data (back propogation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Learning\n",
    "#### Can be applied if no class is specified and any kind of structure is considered \"interesting\"\n",
    "#### Difference to Classification Learning:\n",
    "- Can predict any attribute's value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "#### Finding groups of items that are similar\n",
    "#### Clustering is unsupervised\n",
    "- The class of an example is not known\n",
    "\n",
    "- Sucess often measured subjectively\n",
    "- Clustering techniques apply when there is no calss to be predicted: they perform unsupervised learning\n",
    "- Aim: divide instanves into \"natural\" groups\n",
    "\n",
    "#### Clusters can be:\n",
    "- disjoint vs. overlapping\n",
    "- deterministic vs. probablistic\n",
    "- flat vs. hierarchial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric prediction\n",
    "- Variant of classification learning where \"class\" is numeric (also called regression)\n",
    "- Each instance is described by a fixed predefined set of features, its attributes\n",
    "- But, number of attributes may vary in practice\n",
    "    - Possible solution: \"irrelevant value\" flag\n",
    "- Related Problem: existence of an attribute may depend on value of another one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The k-means algorithm\n",
    "- Step 1: Choose k random cluster centers\n",
    "- Step 2: Assign each instance to its closest cluster center based on Euclidean distance\n",
    "- Step 3: Re-compute cluster centers by computing the average (aka centroid) of the instances pertaining to each cluster\n",
    "- Step 4: If cluster centers have moved, go back to step 2\n",
    "\n",
    "#### This algorithm minimizes the squared Euclidean distance of the instances from their corresponding cluster centers\n",
    "- determines a solution that achieves a local minimum of the squared Euclidean distance\n",
    "\n",
    "#### Equivalent termination criterion: stop when assignment of instances to cluster centers..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means with Tensorflow\n",
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aliases:\n",
    "tf.sub = tf.subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_n = 200\n",
    "clusters_n = 3\n",
    "iterations_n = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Let's generate random data points with a uniform distribution and assign them to a 2D tensor constant. Then, randomly choose initial centroids from the set of data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = tf.constant(np.random.uniform(0, 10, (points_n, 2)))\n",
    "centroids = tf.Variable(tf.slice(tf.random_shuffle(points), [0, 0], [clusters_n, -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Next we want to be able to do element-wise subtraction of points and centroids that are 2D tensors. Because the tensors have different shape, let's expand points and centroids into 3 dimensions, which allows us to use the broadcasting feature of subtraction operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_expanded = tf.expand_dims(points, 0)\n",
    "centroids_expanded = tf.expand_dims(centroids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(points).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Then, calculate the distances between points and centroids and determine the cluster assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = tf.reduce_sum(tf.square)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
